# ELECTRA: 생성자 대신 식별자를 이용하는 훈련 방식

[arxiv](https://arxiv.org/pdf/2003.10555)

## 기존 방식

> MLM 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->Model[Mask Model]-->Masked["[MASK] APPLE"]-->모델-->Original2[An Apple];
> ```
> 일부 단어를 MASK시킨 후 모델에서 [MASK] 토큰을 원래 토큰으로 되돌리는 방식으로 학습함

MLM은 데이터의 15%만 학습하여, 효율성이 부족하다는 문제를 가짐  
또한, 처음 훈련 시에는 MASK하지만, 파인 튜닝할 때는 [MASK] 토큰이 존재하지 않는다는 문제점이 존재함

## Replaced Token Detection

> Replaced Token Detection 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->generator-->Masked["yellow APPLE"]-->discriminator-->Original2[An Apple];
> ```
> 일부 단어를 변경시킨 후 모델에서 변경된 토큰을 원래 토큰으로 되돌리는 방식으로 학습함

이 문제를 해결하기 위해, ELECTRA는 Replaced Token Detection 방식을 사용함  
이 방식은 모든 토큰을 학습에 사용하여, 효율성이 높음  
ELECTRA는 특히 작은 모델에서 강점을 가졌음  
GPT와 비교하여 30배 작은 GPU 자원으로 이겼으며, RoBERTa, XLNET에 비해 4배 적은 자원으로 비슷한 성능을 이뤄냄  
예전에는 transformer 텍스트 인코더를 파인튜닝했고, 우리는 여러 가지를 제거하였고, 이로써 ELECTRA는 모든 토큰들을 학습하고, 이는 BERT보다 ELECTRA가 빠르게 학습할 수 있게 함

### 생성적 적대 신경망 방식(GAN)과의 차이

이 방식은 GAN을 상기시키지만, generator는 가장 비슷하게 토큰을 변경하게 만들어져, 적대적이지 않음  
이렇게 만든 이유는 텍스트에 GAN을 적용하는 것에 대한 어려움 때문임
