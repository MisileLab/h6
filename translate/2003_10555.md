# ELECTRA: 생성자 대신 식별자를 이용하는 훈련 방식

[arxiv](https://arxiv.org/pdf/2003.10555)

## Masked Language Model (ex: BERT)

> MLM 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->Model[Mask Model]-->Masked["[MASK] APPLE"]-->모델-->Original2[An Apple];
> ```
> 일부 단어를 MASK시킨 후 모델에서 [MASK] 토큰을 원래 토큰으로 되돌리는 방식으로 학습함

MLM은 데이터의 15%만 학습하여, 효율성이 부족하다는 문제를 가짐  
또한, 처음 훈련 시에는 MASK하지만, 미세 조정할 때는 [MASK] 토큰이 존재하지 않는다는 문제점이 존재함  
이러한 문제점으로 인해 GPU 자원이 많이 요구되며, 접근성이 낮아짐

## Replaced Token Detection (ELECTRA)

> Replaced Token Detection 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->generator["생성자 (작은 MLM)"]-->Masked["yellow APPLE"]-->식별자-->Original2[An Apple];
> ```
> 일부 단어를 변경시킨 후 모델에서 변경된 토큰을 원래 토큰으로 되돌리는 방식으로 학습함
> 훈련 후에는 생성자를 제거 후, 식별자만 미세 조정함

이 문제를 해결하기 위해, ELECTRA는 Replaced Token Detection 방식을 사용함  
이 방식은 모든 토큰을 학습에 사용하여, 효율성이 높음  
ELECTRA는 특히 작은 모델에서 강점을 가졌음  
GPT와 비교하여 30배 작은 GPU 자원으로 이겼으며, RoBERTa, XLNET에 비해 4배 적은 자원으로 비슷한 성능을 이뤄냄  
전 연구에서는 transformer 텍스트 인코더를 미세 조정하였음  
이 연구에서는 전 연구를 기반으로 필요 없는 것을 제거하였고, 이로써 ELECTRA는 모든 토큰들을 학습하게 되었으며, 이로써 BERT보다 ELECTRA가 빠르게 학습할 수 있게 됨

### 생성적 적대 신경망 방식(GAN)과의 차이

이 방식은 GAN을 상기시키지만, 생성자는 가장 비슷하게 토큰을 변경하게 만들어져, 적대적이지 않음  
이렇게 만든 이유는 생성자의 샘플링에 대해 역전파를 구할 수 없기 때문임

### 적대적 학습의 문제점

- 생성자의 성능이 안 좋음
  - 강화 학습은 텍스트를 생성할 때 샘플에 대한 효율성이 낮음
- 생성자의 출력 분포의 엔트로피가 낮음
  - 확률이 한 토큰에 몰림
  - 결과적으로, 생성자의 출력의 다양성이 낮아짐

### 벤치마크

모델이 절대 성능뿐만 아니라 효율성도 중요함  
그렇기 때문에, 모델을 다양한 사이즈에 맞춰, 다양하게 훈련시켜 평가함  
다른 MLM 기반 모델(ex: BERT, XLNet)과의 비교는 똑같은 조건(모델 크기, 데이터, GPU 자원) 하에 진행되었음  
벤치마크 결과로 보았을 때, ELECTRA는 다른 모델들보다 성능이 높음  
예시를 들자면, ELECTRA-Small은 비슷한 크기의 BERT 모델보다 5점 높았으며, 심지어 매우 큰 크기인 GPT도 넘어섬  
효율성 부문에서는, 적은 파라미터와 25%의 GPU 성능을 사용함에도 불구하고, RoBERTa와 XLNet과 비슷한 성능을 냄  
ELECTRA-Large를 더 훈련시키자, ALBERT를 이겼으며, SQuAD 2.0 부문에서 1위가 됨

#### 실험

아래와 같은 벤치마크를 이용해 평가함
- GLUE
  - textual entailment(텍스트와 문장의 의미 관계를 비슷/다름/관계 없음으로 분류)(RTE,MNLI)
  - question-answer entailment(질문에 올바른 답변을 했는지 분류)(QNLI)
  - paraphrase(두 문장의 의미가 비슷한지 분류)(MRPC)
  - question paraphrase(두 질문이 물어보는 내용이 비슷한지 분류)(QQP)
  - textual similarity(두 문장의 의미적 거리 계산)(STS)
  - sentiment(문장에 나타난 감정이 긍정/부정인지 분류)(sst)
  - lingustic acceptability(문장이 문법적으로 올바른지 분류)(CoLA)
  - STS는 spearman correlation, CoLA는 Matthew's correlation을 지표로 사용
    - 나머지는 정확도를 지표로 사용
- SQuAD
  - 1.1과 2.0 사용
    - 1.1: 질문에 답할 수 있는 특정 구문 선택
    - 2.0: 1.1에 더해서 답변할 수 없는 질문도 추가
  - 지표: Exact-match와 F1 사용
- 데이터
  - BERT와 같은 데이터 사용
    - 데이터: Wikipedia와 Book Corpus에서 가져온 3.3B token
  - ELECTRA-Large는 추가로 XLNet 데이터 사용
    - 데이터: ClueWeb, CommonCrawl, Gigaword에서 가져온 33B token
  - 모든 데이터는 영어임
- 모델
  - 아키텍쳐와 거의 모든 hyperparameter가 BERT와 비슷함
  - GLUE에서는, ELECTRA 위에 XLNet의 간단한 선형 분류 레이어를 추가함
  - SQuAD에서는, ELECTRA 위에 XLNet의 question-answering 레이어를 추가함
    - 이 레이어는 BERT의 레이어보다 약간 복잡함
    - 이유
      - 단독적으로 예측하지 않고, 서로 의존하여 시작과 끝 위치를 예측하기 때문임
      - SQuAD 2.0을 위한 "답변 가능성" 분류 레이어가 존재함
  - 몇몇의 평가 데이터셋은 작기 때문에, 파인 튜닝된 모델의 정확도가 매우 다를 수 있음
    - 파인 튜닝은 랜덤 시드에 의존하기 때문에 정확도가 달라짐
    - 같은 훈련된 체크포인트에서 파인 튜닝된 10개의 모델들의 중앙값을 가지고 비교함

#### 작은 모델

- BERT-Base와의 차이점
  - batch size: 256 -> 128
  - hidden dimension size: 768 -> 256
  - token embedding: 768 -> 128
- BERT-Small도 같은 hyperparameter를 사용해 훈련시킴
  - 1.5M step 사용 = ELECTRA-Small(1M step)과 같은 FLOP
- 자원을 효율적으로 사용하는 ELMo와 GPT와도 비교

##### 모델 확장

여러 가지 확장들을 모델에 추가하여 성능을 증가시킴  
BERT-Base와 같은 모델 크기, 훈련 데이터셋을 사용함

- Weight 공유
  - 생성자와 식별자의 크기가 같다면 weight 공유 가능
    - 하지만 생성자가 더 크기가 작다면 효율적이라는 것이 결과로 나타났기 때문에 임베딩만 공유
  - 500k steps
  - 테스트 결과 -> 기본: 83.6, 토큰 임베딩 공유: 84.3, 모든 weight 공유: 84.4
  - 결과적으로, 모든 weight 공유는 크기가 같아야 한다는 단점이 생기는 것에 비해 크게 성능이 높지 않음
- 작은 생성자
  - 생성자와 식별자의 크기가 같다면 2배의 자원이 필요함
  - 생성자의 레이어 크기를 줄여 모델의 크기를 줄임(hyperparameter 유지)
  - 생성자의 최적 크기는 식별자의 25~50% 크기임
    - 이유: 생성자가 너무 크기가 크면, 식별자에게 너무 어려운 문제를 주고, 효과적으로 훈련하는 것을 막음
