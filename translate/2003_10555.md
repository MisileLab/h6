# ELECTRA: 생성자 대신 식별자를 이용하는 훈련 방식

[arxiv](https://arxiv.org/pdf/2003.10555)

## Masked Language Model (ex: BERT)

> MLM 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->Model[Mask Model]-->Masked["[MASK] APPLE"]-->모델-->Original2[An Apple];
> ```
> 일부 단어를 MASK시킨 후 모델에서 [MASK] 토큰을 원래 토큰으로 되돌리는 방식으로 학습함

MLM은 데이터의 15%만 학습하여, 효율성이 부족하다는 문제를 가짐  
또한, 처음 훈련 시에는 MASK하지만, 미세 조정할 때는 [MASK] 토큰이 존재하지 않는다는 문제점이 존재함  
이러한 문제점으로 인해 GPU 자원이 많이 요구되며, 접근성이 낮아짐

## Replaced Token Detection (ELECTRA)

> Replaced Token Detection 방식
> ```mermaid
> graph LR;
> Original[An Apple]-->generator["생성자 (작은 MLM)"]-->Masked["yellow APPLE"]-->식별자-->Original2[An Apple];
> ```
> 일부 단어를 변경시킨 후 모델에서 변경된 토큰을 원래 토큰으로 되돌리는 방식으로 학습함
> 훈련 후에는 생성자를 제거 후, 식별자만 미세 조정함

이 문제를 해결하기 위해, ELECTRA는 Replaced Token Detection 방식을 사용함  
이 방식은 모든 토큰을 학습에 사용하여, 효율성이 높음  
ELECTRA는 특히 작은 모델에서 강점을 가졌음  
GPT와 비교하여 30배 작은 GPU 자원으로 이겼으며, RoBERTa, XLNET에 비해 4배 적은 자원으로 비슷한 성능을 이뤄냄  
전 연구에서는 transformer 텍스트 인코더를 미세 조정하였음  
이 연구에서는 전 연구를 기반으로 필요 없는 것을 제거하였고, 이로써 ELECTRA는 모든 토큰들을 학습하게 되었으며, 이로써 BERT보다 ELECTRA가 빠르게 학습할 수 있게 됨

### 생성적 적대 신경망 방식(GAN)과의 차이

이 방식은 GAN을 상기시키지만, 생성자는 가장 비슷하게 토큰을 변경하게 만들어져, 적대적이지 않음  
이렇게 만든 이유는 생성자의 샘플링에 대해 역전파를 구할 수 없기 때문임

### 벤치마크

저자는 모델이 절대 성능뿐만 아니라 효율성도 중요하다고 평가했기 때문에, 모델을 다양한 사이즈에 맞춰, 다양하게 훈련시켜 평가함  
세부적으로 보자면, GLUE와 SQuAD를 이용하여 평가하였음  
다른 MLM 기반 모델(ex: BERT, XLNet)과의 비교는 똑같은 조건(모델 크기, 데이터, GPU 자원) 하에 진행되었음  
벤치마크 결과로 보았을 때, ELECTRA는 다른 모델들보다 성능이 높음  
예시를 들자면, ELECTRA-Small은 비슷한 크기의 BERT 모델보다 5점 높았으며, 심지어 매우 큰 크기인 GPT도 넘어섬  
효율성 부문에서는, 적은 파라미터와 25%의 GPU 성능을 사용함에도 불구하고, RoBERTa와 XLNet과 비슷한 성능을 냄  
ELECTRA-Large를 더 훈련시키자, ALBERT를 이겼으며, SQuAD 2.0 부문에서 1위가 됨