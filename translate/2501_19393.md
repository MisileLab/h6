# s1, 간단한 test-time 스케일링

[arxiv](https://arxiv.org/abs/2501.19393)

## 논문 초록 (Abstract)

Test-time scaling은 성능을 향상시키기 위해 추가 test-time 자원(compute)을 사용하는 유망한 언어 모델의 새로운 접근 방식입니다.  
최근, OpenAI의 o1 모델은 이 방식의 능력을 보여주었지만, 공개적으로 방법을 공유하지 않았고, 많은 복제(replication) 시도로 이어지게 되었습니다.  
우리는 test-time scaling과 강력한 추론 성능을 달성하기 위한 간단한 접근 방식을 찾고자 합니다.  
먼저, 우리는 난이도, 다양성, 그리고 품질이라는 세 가지 기준을 기반으로 검증된 소규모 데이터셋 s1K를 구축하였고, 여기에는 1,000개의 질문과 추론 과정이 포함되었습니다.  
다음으로, 우리는 예산 강제(budget forcing)를 개발하여 모델의 생각을 강제로 중단하거나 모델이 종료하려고 할 때 모델의 생성 결과(generation)에 “Wait”을 여러 번 추가하는 것으로 생각을 연장시켜 test-time 자원을 제어하였습니다.  
이를 통해 모델이 답변을 재검토하고, 종종 잘못된 추론 단계를 고치게 유도할 수 있습니다.  
Qwen2.5 32B-Instruct을 s1K를 사용하여 supervised fine-tuning하였고, 예산 강제를 추가한 결과, 우리의 모델 s1-32B는 수학 벤치마크(MATH and AIME24)에서 o1-preview와 비교하여 27%의 성능 향상을 보였습니다.
또한, 예산 강제를 사용한 s1-32B의 스케일링을 통해 test-time 개입(intervention) 없이도 성능을 더욱 향상(AIME24: 50% -> 57%)시킬 수 있었습니다.
우리의 모델, 데이터, 그리고 코드는 [https://github.com/simplescaling/s1](https://github.com/simplescaling/s1)에 오픈 소스로 공개되어 있습니다.

## Introduction

지난 몇 년 동안 언어 모델들(LMs)의 성능 향상은 self-supervised pretraining([Ka plan et al., 2020;](https://arxiv.org/pdf/2501.19393#cite.kaplan2020scalinglawsneurallanguage) [Hoffmann et al., 2022](https://arxiv.org/pdf/2501.19393#cite.hoffmann2022trainingcomputeoptimallargelanguage))을 사용하여 train-time 자원을 증가(scaling up)시키는 것에 크게 의존해왔습니다.  
이러한 강력한 모델들의 출현(creation)은 그 위에 구축된 새로운 스케일링 방식인 test-time 스케일링의 기반을 마련(set the stage)했습니다.  
이 방식의 목표는 더 좋은 결과를 얻기 위해 test-time 자원을 증가시키는 것입니다.  
이 방식(idea)를 탐구하는데 많은 연구([Snell et al., 2024;](https://arxiv.org/pdf/2501.19393#cite.snell2024scalingllmtesttimecompute) [Welleck et al., 2024](https://arxiv.org/pdf/2501.19393#cite.welleck2024decodingmetagenerationinferencetimealgorithms))가 진행되었으며, 최근 이 방식의 실효성(viability)이 OpenAI o1([OpenAI, 2024](https://arxiv.org/pdf/2501.19393#cite.o1))에 의해 검증되었습니다.  
o1은 test-time 자원을 증가시켜 일관된 성능 향상을 보이고 강력한 추론 능력을 입증했습니다.  
이 결과는 몬테 카를로 트리 탐색([Gao et al., 2024b;](https://arxiv.org/pdf/2501.19393#cite.gao2024interpretablecontrastivemontecarlo) [Zhang et al., 2024a](https://arxiv.org/pdf/2501.19393#cite.zhang2024o1codero1replicationcoding)), multi-agent 방식([Qin et al., 2024](https://arxiv.org/pdf/2501.19393#cite.qin2024o1replicationjourneystrategic)), 그리고 다른 방식들([Wang et al., 2024a;](https://arxiv.org/pdf/2501.19393#cite.wang2024drto1optimizeddeepreasoning) [Huang et al., 2024b;](https://arxiv.org/pdf/2501.19393#cite.huang2024o1replicationjourney) [2025](https://arxiv.org/pdf/2501.19393#cite.huang2025o1replicationjourney))과 같은 기술에 기반해 모델을 복제하려는 여러 시도로 이어졌습니다.  
그 시도들 중에, DeepSeek R1([DeepSeek-AI et al., 2025](https://arxiv.org/pdf/2501.19393#cite.r1))은 성공적으로 o1과 비슷한 성능을 이끌었고(replicated), 또한 수백만개의 샘플과 여러 개의 훈련 과정을 사용한 강화 학습을 사용했습니다.  
그러나, 많은 복제 시도에도 불구하고, 아무 시도도 명확한 test-time scailing의 행동을 공개적으로 복제하지 못하였습니다.  
따라서, 우리는 질문했습니다: test-time scailing과 강력한 추론 능력을 얻는 가장 간단한 접근 방식은 무엇인가?  
우리는 예산 강제라고 부르는 간단한 test-time 기술을 이용한 추론 시간 제어와 다음 토큰 예상을 포함한 1000개의 샘플만을 이용해 훈련하였고, 그것은 test-time 자원과 비례하여 성능을 향상시키는 강력한 추론 모델로 이끌었습니다.  
구체적으로는, 우리는 1000개의 신중하게 선별된 질문과 Gemini Thinking Experimental([Google, 2024](https://arxiv.org/pdf/2501.19393#cite.geminithinking))에서 증류된 추론 기록(traces)과 답으로 구성된 s1K를 구축했습니다.  
우리는 16개의 H100을 이용하여 26분만에 훈련이 가능한 작은 데이터셋을 이용해 미리 훈련된 모델에서 supervised fine-tuning하였습니다.
